{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pgvector quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dotenv\n",
    "\n",
    "\n",
    "# Reload the variables in your '.env' file (override the existing variables)\n",
    "dotenv.load_dotenv(\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger set up with log level: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malcolm/.pyenv/versions/3.10.14/envs/rag_techniques3/lib/python3.10/site-packages/deepeval/__init__.py:45: UserWarning: You are using deepeval version 0.21.70, however version 1.1.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<loguru.logger handlers=[(id=1, level=10, sink=stdout)]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-29 17:51:08.602\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_57', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 51, 8, 602678, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_57', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 51, 7, 242000, tzinfo=tzutc()), 'msg_id': '6cdc8caf-72ad-4cdb-aa69-bd398975aa3a', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 3, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 51, 7, 245419, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..'))) # Add the parent directory to the path since we work with notebooks\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "\n",
    "from logging_utils import get_logger, global_log_config\n",
    "\n",
    "\n",
    "global_log_config(\n",
    "    log_level=logging.getLevelName(\"DEBUG\"),\n",
    "    json=False,\n",
    ")\n",
    "\n",
    "\n",
    "# # Load environment variables from a .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set the OpenAI API key environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-29 17:51:35.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:419\u001b[0m | \u001b[34m\u001b[1m\n",
      "*** MESSAGE TYPE:execute_request***\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:51:35.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:420\u001b[0m | \u001b[34m\u001b[1m   Content: {'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': False, 'code': '# SOURCE: https://www.youtube.com/watch?v=Ff3tJ4pJEa4\\nimport pinecone\\nfrom dotenv import load_dotenv\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Pinecone\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.vectorstores.pgvector import PGVector\\nfrom pgvector_service import PgvectorService\\nimport os\\nimport time'}\n",
      "   --->\n",
      "   \u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:51:35.075\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:429\u001b[0m | \u001b[34m\u001b[1mexecute_request: {'header': {'date': datetime.datetime(2024, 8, 29, 21, 51, 35, 73000, tzinfo=tzutc()), 'msg_id': '95a16cc6-a7d5-46e3-b794-bf5678d7d235', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'msg_id': '95a16cc6-a7d5-46e3-b794-bf5678d7d235', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/malcolm/dev/bossjones/RAG_Techniques/notebooks/pgvector/pgvector_quickstart.ipynb#X40sZmlsZQ%3D%3D'}, 'content': {'silent': False, 'store_history': True, 'user_expressions': {}, 'allow_stdin': True, 'stop_on_error': False, 'code': '# SOURCE: https://www.youtube.com/watch?v=Ff3tJ4pJEa4\\nimport pinecone\\nfrom dotenv import load_dotenv\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Pinecone\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.vectorstores.pgvector import PGVector\\nfrom pgvector_service import PgvectorService\\nimport os\\nimport time'}, 'buffers': []}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:51:36.147\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_65', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 51, 36, 147330, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_65', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 51, 35, 73000, tzinfo=tzutc()), 'msg_id': '95a16cc6-a7d5-46e3-b794-bf5678d7d235', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 4, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 51, 35, 75756, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:419\u001b[0m | \u001b[34m\u001b[1m\n",
      "*** MESSAGE TYPE:execute_request***\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:420\u001b[0m | \u001b[34m\u001b[1m   Content: {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.2\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"loader = TextLoader(\\\\n    \\\\\".../data/The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas.txt\\\\\"\\\\n)\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\\\\ndocs = text_splitter.split_documents(documents)\\\\n\\\\nembeddings = OpenAIEmbeddings()\\\\n\\\\nquery = \\\\\"The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas\\\\\"\\\\n\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nFirst, we compare to Pinecone, a managed vector store service.\\\\n\\\\n\\\\\"\\\\\"\\\\\"\", 27)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.2.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.2.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}\n",
      "   --->\n",
      "   \u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:429\u001b[0m | \u001b[34m\u001b[1mexecute_request: {'header': {'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 381000, tzinfo=tzutc()), 'msg_id': 'a96dc264-8f05-4d09-b733-552a08b662bf', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'msg_id': 'a96dc264-8f05-4d09-b733-552a08b662bf', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.2\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"loader = TextLoader(\\\\n    \\\\\".../data/The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas.txt\\\\\"\\\\n)\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\\\\ndocs = text_splitter.split_documents(documents)\\\\n\\\\nembeddings = OpenAIEmbeddings()\\\\n\\\\nquery = \\\\\"The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas\\\\\"\\\\n\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nFirst, we compare to Pinecone, a managed vector store service.\\\\n\\\\n\\\\\"\\\\\"\\\\\"\", 27)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.2.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.2.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}, 'buffers': []}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.397\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_75', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 396979, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_75', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 381000, tzinfo=tzutc()), 'msg_id': 'a96dc264-8f05-4d09-b733-552a08b662bf', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 4, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 52, 14, 388844, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:419\u001b[0m | \u001b[34m\u001b[1m\n",
      "*** MESSAGE TYPE:execute_request***\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:420\u001b[0m | \u001b[34m\u001b[1m   Content: {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.3\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"loader = TextLoader(\\\\n    \\\\\"..../data/The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas.txt\\\\\"\\\\n)\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\\\\ndocs = text_splitter.split_documents(documents)\\\\n\\\\nembeddings = OpenAIEmbeddings()\\\\n\\\\nquery = \\\\\"The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas\\\\\"\\\\n\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nFirst, we compare to Pinecone, a managed vector store service.\\\\n\\\\n\\\\\"\\\\\"\\\\\"\", 28)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.3.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.3.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}\n",
      "   --->\n",
      "   \u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:429\u001b[0m | \u001b[34m\u001b[1mexecute_request: {'header': {'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 529000, tzinfo=tzutc()), 'msg_id': 'efade72a-2381-44b5-a30d-b23ab19240bb', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'msg_id': 'efade72a-2381-44b5-a30d-b23ab19240bb', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.3\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"loader = TextLoader(\\\\n    \\\\\"..../data/The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas.txt\\\\\"\\\\n)\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\\\\ndocs = text_splitter.split_documents(documents)\\\\n\\\\nembeddings = OpenAIEmbeddings()\\\\n\\\\nquery = \\\\\"The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas\\\\\"\\\\n\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nFirst, we compare to Pinecone, a managed vector store service.\\\\n\\\\n\\\\\"\\\\\"\\\\\"\", 28)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.3.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.3.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}, 'buffers': []}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:52:14.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_85', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 538204, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_85', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 52, 14, 529000, tzinfo=tzutc()), 'msg_id': 'efade72a-2381-44b5-a30d-b23ab19240bb', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 4, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 52, 14, 531460, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:419\u001b[0m | \u001b[34m\u001b[1m\n",
      "*** MESSAGE TYPE:execute_request***\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:420\u001b[0m | \u001b[34m\u001b[1m   Content: {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.4\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"\\\\nloader = TextLoader(\\\\\".../data/The Project Gutenberg eBook of Romeo and Juliet.txt\\\\\")\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\\\nnew_docs = text_splitter.split_documents(documents)\\\\n\\\\n\\\\nCOLLECTION_NAME_2 = \\\\\"The Project Gutenberg eBook of Romeo and Juliet\\\\\"\\\\n\\\\ndb = PGVector.from_documents(\\\\n    embedding=embeddings,\\\\n    documents=new_docs,\\\\n    collection_name=COLLECTION_NAME_2,\\\\n    connection_string=CONNECTION_STRING,\\\\n    pre_delete_collection=False,\\\\n)\", 23)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.4.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.4.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}\n",
      "   --->\n",
      "   \u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:429\u001b[0m | \u001b[34m\u001b[1mexecute_request: {'header': {'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 433000, tzinfo=tzutc()), 'msg_id': '9cf7dab9-24e4-4560-957d-c7c37fe41287', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'msg_id': '9cf7dab9-24e4-4560-957d-c7c37fe41287', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.4\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"\\\\nloader = TextLoader(\\\\\".../data/The Project Gutenberg eBook of Romeo and Juliet.txt\\\\\")\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\\\nnew_docs = text_splitter.split_documents(documents)\\\\n\\\\n\\\\nCOLLECTION_NAME_2 = \\\\\"The Project Gutenberg eBook of Romeo and Juliet\\\\\"\\\\n\\\\ndb = PGVector.from_documents(\\\\n    embedding=embeddings,\\\\n    documents=new_docs,\\\\n    collection_name=COLLECTION_NAME_2,\\\\n    connection_string=CONNECTION_STRING,\\\\n    pre_delete_collection=False,\\\\n)\", 23)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.4.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.4.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}, 'buffers': []}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.447\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_95', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 447148, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_95', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 433000, tzinfo=tzutc()), 'msg_id': '9cf7dab9-24e4-4560-957d-c7c37fe41287', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 4, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 55, 17, 438308, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.565\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:419\u001b[0m | \u001b[34m\u001b[1m\n",
      "*** MESSAGE TYPE:execute_request***\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.565\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:420\u001b[0m | \u001b[34m\u001b[1m   Content: {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.5\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"\\\\nloader = TextLoader(\\\\\"..../data/The Project Gutenberg eBook of Romeo and Juliet.txt\\\\\")\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\\\nnew_docs = text_splitter.split_documents(documents)\\\\n\\\\n\\\\nCOLLECTION_NAME_2 = \\\\\"The Project Gutenberg eBook of Romeo and Juliet\\\\\"\\\\n\\\\ndb = PGVector.from_documents(\\\\n    embedding=embeddings,\\\\n    documents=new_docs,\\\\n    collection_name=COLLECTION_NAME_2,\\\\n    connection_string=CONNECTION_STRING,\\\\n    pre_delete_collection=False,\\\\n)\", 24)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.5.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.5.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}\n",
      "   --->\n",
      "   \u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.565\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mdispatch_shell\u001b[0m - \u001b[35mkernelbase.py:429\u001b[0m | \u001b[34m\u001b[1mexecute_request: {'header': {'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 563000, tzinfo=tzutc()), 'msg_id': '1ca19e0b-8d07-43f0-b064-8f8a04d32e9b', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'msg_id': '1ca19e0b-8d07-43f0-b064-8f8a04d32e9b', 'msg_type': 'execute_request', 'parent_header': {}, 'metadata': {}, 'content': {'silent': False, 'store_history': False, 'user_expressions': {}, 'allow_stdin': False, 'stop_on_error': False, 'code': 'def __jupyter_exec_background__():\\n    from IPython.display import display\\n    from threading import Thread\\n    from traceback import format_exc\\n\\n    # First send a dummy response to get the display id.\\n    # Later we\\'ll send the real response with the actual data.\\n    # And that can happen much later even after the execution completes,\\n    # as that response will be sent from a bg thread.\\n    output = display({\"application/vnd.vscode.bg.execution.5\": \"\"}, raw=True, display_id=True)\\n\\n    def do_implementation():\\n        return get_ipython().kernel.do_complete(\"\\\\nloader = TextLoader(\\\\\"..../data/The Project Gutenberg eBook of Romeo and Juliet.txt\\\\\")\\\\ndocuments = loader.load()\\\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\\\nnew_docs = text_splitter.split_documents(documents)\\\\n\\\\n\\\\nCOLLECTION_NAME_2 = \\\\\"The Project Gutenberg eBook of Romeo and Juliet\\\\\"\\\\n\\\\ndb = PGVector.from_documents(\\\\n    embedding=embeddings,\\\\n    documents=new_docs,\\\\n    collection_name=COLLECTION_NAME_2,\\\\n    connection_string=CONNECTION_STRING,\\\\n    pre_delete_collection=False,\\\\n)\", 24)\\n\\n    def bg_main():\\n        try:\\n            output.update({\"application/vnd.vscode.bg.execution.5.result\": do_implementation()}, raw=True)\\n        except Exception as e:\\n            output.update({\"application/vnd.vscode.bg.execution.5.error\": format_exc()}, raw=True)\\n\\n\\n    Thread(target=bg_main, daemon=True).start()\\n\\n\\n__jupyter_exec_background__()\\ndel __jupyter_exec_background__'}, 'buffers': []}\u001b[0m | {}\n",
      "\u001b[32m2024-08-29 17:55:17.571\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mipykernel.kernelbase\u001b[0m:\u001b[36mexecute_request\u001b[0m - \u001b[35mkernelbase.py:802\u001b[0m | \u001b[34m\u001b[1m{'header': {'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_105', 'msg_type': 'execute_reply', 'username': 'malcolm', 'session': '4b11725e-71088fba0b611b02bc8a66c1', 'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 571561, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '4b11725e-71088fba0b611b02bc8a66c1_98861_105', 'msg_type': 'execute_reply', 'parent_header': {'date': datetime.datetime(2024, 8, 29, 21, 55, 17, 563000, tzinfo=tzutc()), 'msg_id': '1ca19e0b-8d07-43f0-b064-8f8a04d32e9b', 'msg_type': 'execute_request', 'session': 'c3b87cad-6ae8-4acf-91d0-b3ff94db4eba', 'username': 'fe1b0bb7-37c8-412c-8e1a-806c493c8205', 'version': '5.2'}, 'content': {'status': 'ok', 'execution_count': 4, 'user_expressions': {}, 'payload': []}, 'metadata': {'started': datetime.datetime(2024, 8, 29, 21, 55, 17, 565633, tzinfo=datetime.timezone.utc), 'dependencies_met': True, 'engine': '9e4fa85b-f34b-4635-bee8-48043cbe0b82', 'status': 'ok'}, 'tracker': <zmq.sugar.tracker.MessageTracker object at 0x10504bf10>}\u001b[0m | {}\n"
     ]
    }
   ],
   "source": [
    "# SOURCE: https://www.youtube.com/watch?v=Ff3tJ4pJEa4\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from pgvector_service import PgvectorService\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\n",
    "    \"../../data/The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas.txt\"\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "query = \"The Project Gutenberg eBook of A Christmas Carol in Prose; Being a Ghost Story of Christmas\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "First, we compare to Pinecone, a managed vector store service.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create / Load the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\")\n",
    ")\n",
    "\n",
    "index_name = \"demo-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=1536)\n",
    "    pinecone_docsearch = Pinecone.from_documents(\n",
    "        docs, embeddings, index_name=\"demo-index\"\n",
    "    )\n",
    "else:\n",
    "    pinecone_docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the index with LanChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_pinecone(docsearch, query):\n",
    "    docs = docsearch.similarity_search(query, k=4)\n",
    "    result = docs[0].page_content\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_average_execution_time(func, *args, **kwargs):\n",
    "    total_execution_time = 0\n",
    "    num_runs = 10\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)  # Execute the function with its arguments\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        total_execution_time += execution_time\n",
    "    average_execution_time = round(total_execution_time / num_runs, 2)\n",
    "    print(result)\n",
    "    print(\n",
    "        f\"\\nThe function took an average of {average_execution_time} seconds to execute.\"\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "calculate_average_execution_time(\n",
    "    run_query_pinecone, docsearch=pinecone_docsearch, query=query\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now, we compare to PGVector, an open source vector store service.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a PGVector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Donwload postgresql to run locally:\n",
    "https://www.postgresql.org/download/\n",
    "\n",
    "How to install the pgvector extension:\n",
    "https://github.com/pgvector/pgvector\n",
    "\n",
    "Fix common installation issues:\n",
    "https://github.com/pgvector/pgvector?tab=readme-ov-file#installation-notes\n",
    "\"\"\"\n",
    "\n",
    "COLLECTION_NAME = \"The Project Gutenberg eBook of A Christmas Carol in Prose\"\n",
    "\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "    driver=os.environ.get(\"PGVECTOR_DRIVER\", \"psycopg3\"),\n",
    "    host=os.environ.get(\"PGVECTOR_HOST\", \"localhost\"),\n",
    "    port=int(os.environ.get(\"PGVECTOR_PORT\", \"6432\")),\n",
    "    database=os.environ.get(\"PGVECTOR_DATABASE\", \"langchain\"),\n",
    "    user=os.environ.get(\"PGVECTOR_USER\", \"langchain\"),\n",
    "    password=os.environ.get(\"PGVECTOR_PASSWORD\", \"langchain\"),\n",
    ")\n",
    "\n",
    "# create the store\n",
    "db = PGVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    pre_delete_collection=False,\n",
    ")\n",
    "\n",
    "# load the store\n",
    "pgvector_docsearch = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the index with PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_query_pgvector(docsearch, query):\n",
    "    docs = docsearch.similarity_search(query, k=4)\n",
    "    result = docs[0].page_content\n",
    "    return result\n",
    "\n",
    "\n",
    "calculate_average_execution_time(\n",
    "    run_query_pgvector, docsearch=pgvector_docsearch, query=query\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more collections to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = TextLoader(\"../../data/The Project Gutenberg eBook of Romeo and Juliet.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "new_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "COLLECTION_NAME_2 = \"The Project Gutenberg eBook of Romeo and Juliet\"\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=new_docs,\n",
    "    collection_name=COLLECTION_NAME_2,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    pre_delete_collection=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the index with multiple collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pg = PgvectorService(CONNECTION_STRING)\n",
    "\n",
    "\n",
    "def run_query_multi_pgvector(docsearch, query):\n",
    "    docs = docsearch.custom_similarity_search_with_scores(query, k=4)\n",
    "    result = docs[0][0].page_content\n",
    "    print(result)\n",
    "\n",
    "\n",
    "run_query_multi_pgvector(pg, query)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Delete the collection\n",
    "# --------------------------------------------------------------\n",
    "pg.delete_collection(COLLECTION_NAME)\n",
    "pg.delete_collection(COLLECTION_NAME_2)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Update the collection\n",
    "# --------------------------------------------------------------\n",
    "pg.update_collection(docs=docs, collection_name=COLLECTION_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
